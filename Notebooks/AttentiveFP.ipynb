{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6b631d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn.models import AttentiveFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52ce571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenFeatures(object):\n",
    "    def __init__(self):\n",
    "        self.symbols = [\n",
    "            'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br',\n",
    "            'Te', 'I', 'At', 'other'\n",
    "        ]\n",
    "\n",
    "        self.hybridizations = [\n",
    "            Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2,\n",
    "            Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D,\n",
    "            Chem.rdchem.HybridizationType.SP3D2,\n",
    "            'other',\n",
    "        ]\n",
    "\n",
    "        self.stereos = [\n",
    "            Chem.rdchem.BondStereo.STEREONONE,\n",
    "            Chem.rdchem.BondStereo.STEREOANY,\n",
    "            Chem.rdchem.BondStereo.STEREOZ,\n",
    "            Chem.rdchem.BondStereo.STEREOE,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Generate AttentiveFP features according to Table 1.\n",
    "        mol = Chem.MolFromSmiles(data.smiles)\n",
    "\n",
    "        xs = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            symbol = [0.] * len(self.symbols)\n",
    "            symbol[self.symbols.index(atom.GetSymbol())] = 1.\n",
    "            degree = [0.] * 6\n",
    "            degree[atom.GetDegree()] = 1.\n",
    "            formal_charge = atom.GetFormalCharge()\n",
    "            radical_electrons = atom.GetNumRadicalElectrons()\n",
    "            hybridization = [0.] * len(self.hybridizations)\n",
    "            hybridization[self.hybridizations.index(\n",
    "                atom.GetHybridization())] = 1.\n",
    "            aromaticity = 1. if atom.GetIsAromatic() else 0.\n",
    "            hydrogens = [0.] * 5\n",
    "            hydrogens[atom.GetTotalNumHs()] = 1.\n",
    "            chirality = 1. if atom.HasProp('_ChiralityPossible') else 0.\n",
    "            chirality_type = [0.] * 2\n",
    "            if atom.HasProp('_CIPCode'):\n",
    "                chirality_type[['R', 'S'].index(atom.GetProp('_CIPCode'))] = 1.\n",
    "\n",
    "            x = torch.tensor(symbol + degree + [formal_charge] +\n",
    "                             [radical_electrons] + hybridization +\n",
    "                             [aromaticity] + hydrogens + [chirality] +\n",
    "                             chirality_type)\n",
    "            xs.append(x)\n",
    "\n",
    "        data.x = torch.stack(xs, dim=0)\n",
    "\n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "        for bond in mol.GetBonds():\n",
    "            edge_indices += [[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]]\n",
    "            edge_indices += [[bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]]\n",
    "\n",
    "            bond_type = bond.GetBondType()\n",
    "            single = 1. if bond_type == Chem.rdchem.BondType.SINGLE else 0.\n",
    "            double = 1. if bond_type == Chem.rdchem.BondType.DOUBLE else 0.\n",
    "            triple = 1. if bond_type == Chem.rdchem.BondType.TRIPLE else 0.\n",
    "            aromatic = 1. if bond_type == Chem.rdchem.BondType.AROMATIC else 0.\n",
    "            conjugation = 1. if bond.GetIsConjugated() else 0.\n",
    "            ring = 1. if bond.IsInRing() else 0.\n",
    "            stereo = [0.] * 4\n",
    "            stereo[self.stereos.index(bond.GetStereo())] = 1.\n",
    "\n",
    "            edge_attr = torch.tensor(\n",
    "                [single, double, triple, aromatic, conjugation, ring] + stereo)\n",
    "\n",
    "            edge_attrs += [edge_attr, edge_attr]\n",
    "\n",
    "        if len(edge_attrs) == 0:\n",
    "            data.edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "            data.edge_attr = torch.zeros((0, 10), dtype=torch.float)\n",
    "        else:\n",
    "            data.edge_index = torch.tensor(edge_indices).t().contiguous()\n",
    "            data.edge_attr = torch.stack(edge_attrs, dim=0)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad1e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MoleculeNet('./ESOL', name='ESOL', pre_transform=GenFeatures()).shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948de95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset) // 10\n",
    "val_dataset   = dataset[:N]\n",
    "test_dataset  = dataset[N:2 * N]\n",
    "train_dataset = dataset[2 * N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1254241",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=200)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d60fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4580c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.6700]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b93878a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[6, 10], edge_index=[2, 6], smiles=\"ClCC#N\", x=[4, 39], y=[1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b06494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentiveFP(\n",
    "    in_channels=val_dataset.num_node_features,\n",
    "    hidden_channels=200,\n",
    "    out_channels=val_dataset.num_classes,\n",
    "    edge_dim=10,\n",
    "    num_layers=2,\n",
    "    num_timesteps=2,\n",
    "    dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "febb748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=10**-2.5, weight_decay=10**-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0fc1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "    return sqrt(total_loss / total_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dac3ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    mse = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        mse.append(F.mse_loss(out, data.y, reduction='none').cpu())\n",
    "    return float(torch.cat(mse, dim=0).mean().sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a69453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 3.2639 Val: 2.4595 Test: 2.5414\n",
      "Epoch: 002, Loss: 2.4051 Val: 1.7341 Test: 1.7510\n",
      "Epoch: 003, Loss: 1.8597 Val: 1.7916 Test: 1.7675\n",
      "Epoch: 004, Loss: 1.8131 Val: 1.6843 Test: 1.7599\n",
      "Epoch: 005, Loss: 1.7144 Val: 1.6425 Test: 1.6321\n",
      "Epoch: 006, Loss: 1.6561 Val: 1.5783 Test: 1.5231\n",
      "Epoch: 007, Loss: 1.5744 Val: 1.4925 Test: 1.5245\n",
      "Epoch: 008, Loss: 1.4497 Val: 1.2012 Test: 1.2388\n",
      "Epoch: 009, Loss: 1.2594 Val: 1.0138 Test: 1.1322\n",
      "Epoch: 010, Loss: 1.1547 Val: 1.0295 Test: 0.9961\n",
      "Epoch: 011, Loss: 1.1126 Val: 1.0019 Test: 0.9567\n",
      "Epoch: 012, Loss: 1.1113 Val: 1.0398 Test: 1.0505\n",
      "Epoch: 013, Loss: 1.0992 Val: 0.9127 Test: 0.9240\n",
      "Epoch: 014, Loss: 1.0456 Val: 1.0245 Test: 1.0030\n",
      "Epoch: 015, Loss: 1.0229 Val: 0.9596 Test: 0.9990\n",
      "Epoch: 016, Loss: 0.9633 Val: 0.9566 Test: 0.9076\n",
      "Epoch: 017, Loss: 0.9547 Val: 0.8932 Test: 0.8839\n",
      "Epoch: 018, Loss: 0.9290 Val: 0.8758 Test: 0.8364\n",
      "Epoch: 019, Loss: 0.9149 Val: 0.9197 Test: 0.9234\n",
      "Epoch: 020, Loss: 0.8841 Val: 0.7916 Test: 0.9062\n",
      "Epoch: 021, Loss: 0.8928 Val: 0.8042 Test: 0.7350\n",
      "Epoch: 022, Loss: 0.8365 Val: 0.8121 Test: 0.8259\n",
      "Epoch: 023, Loss: 0.8070 Val: 0.7911 Test: 0.7547\n",
      "Epoch: 024, Loss: 0.8008 Val: 0.7668 Test: 0.8371\n",
      "Epoch: 025, Loss: 0.8232 Val: 0.9101 Test: 0.8439\n",
      "Epoch: 026, Loss: 0.8568 Val: 0.8938 Test: 0.8656\n",
      "Epoch: 027, Loss: 0.8206 Val: 0.8626 Test: 0.8271\n",
      "Epoch: 028, Loss: 0.7910 Val: 0.7842 Test: 0.8222\n",
      "Epoch: 029, Loss: 0.7673 Val: 0.8046 Test: 0.7936\n",
      "Epoch: 030, Loss: 0.7395 Val: 0.7864 Test: 0.7572\n",
      "Epoch: 031, Loss: 0.7400 Val: 0.6984 Test: 0.7294\n",
      "Epoch: 032, Loss: 0.7355 Val: 0.7675 Test: 0.7317\n",
      "Epoch: 033, Loss: 0.7065 Val: 0.7918 Test: 0.7861\n",
      "Epoch: 034, Loss: 0.7199 Val: 0.7436 Test: 0.6735\n",
      "Epoch: 035, Loss: 0.7044 Val: 0.7092 Test: 0.7843\n",
      "Epoch: 036, Loss: 0.6984 Val: 0.7802 Test: 0.7358\n",
      "Epoch: 037, Loss: 0.7135 Val: 0.7840 Test: 0.6775\n",
      "Epoch: 038, Loss: 0.7116 Val: 0.7236 Test: 0.7072\n",
      "Epoch: 039, Loss: 0.6981 Val: 0.8339 Test: 0.7037\n",
      "Epoch: 040, Loss: 0.6559 Val: 0.7819 Test: 0.6970\n",
      "Epoch: 041, Loss: 0.6720 Val: 0.7591 Test: 0.7417\n",
      "Epoch: 042, Loss: 0.6652 Val: 0.8300 Test: 0.7769\n",
      "Epoch: 043, Loss: 0.6799 Val: 0.8454 Test: 0.7012\n",
      "Epoch: 044, Loss: 0.6798 Val: 0.6601 Test: 0.7411\n",
      "Epoch: 045, Loss: 0.6374 Val: 0.7302 Test: 0.7157\n",
      "Epoch: 046, Loss: 0.6200 Val: 0.6592 Test: 0.7312\n",
      "Epoch: 047, Loss: 0.6522 Val: 0.7062 Test: 0.7044\n",
      "Epoch: 048, Loss: 0.6289 Val: 0.8202 Test: 0.7379\n",
      "Epoch: 049, Loss: 0.6336 Val: 0.7391 Test: 0.6954\n",
      "Epoch: 050, Loss: 0.6211 Val: 0.6772 Test: 0.6578\n",
      "Epoch: 051, Loss: 0.6323 Val: 0.6953 Test: 0.6728\n",
      "Epoch: 052, Loss: 0.6348 Val: 0.6886 Test: 0.7563\n",
      "Epoch: 053, Loss: 0.6387 Val: 0.6878 Test: 0.7010\n",
      "Epoch: 054, Loss: 0.6189 Val: 0.7001 Test: 0.5867\n",
      "Epoch: 055, Loss: 0.6102 Val: 0.6529 Test: 0.7072\n",
      "Epoch: 056, Loss: 0.5944 Val: 0.6454 Test: 0.6899\n",
      "Epoch: 057, Loss: 0.5721 Val: 0.7282 Test: 0.6626\n",
      "Epoch: 058, Loss: 0.5861 Val: 0.7565 Test: 0.7364\n",
      "Epoch: 059, Loss: 0.5966 Val: 0.7701 Test: 0.7763\n",
      "Epoch: 060, Loss: 0.6085 Val: 0.7263 Test: 0.7420\n",
      "Epoch: 061, Loss: 0.5857 Val: 0.8082 Test: 0.7540\n",
      "Epoch: 062, Loss: 0.5874 Val: 0.6689 Test: 0.7111\n",
      "Epoch: 063, Loss: 0.5606 Val: 0.7122 Test: 0.7641\n",
      "Epoch: 064, Loss: 0.5985 Val: 0.6809 Test: 0.6601\n",
      "Epoch: 065, Loss: 0.5949 Val: 0.7127 Test: 0.6861\n",
      "Epoch: 066, Loss: 0.5582 Val: 0.7086 Test: 0.7271\n",
      "Epoch: 067, Loss: 0.5640 Val: 0.7079 Test: 0.7253\n",
      "Epoch: 068, Loss: 0.5507 Val: 0.6662 Test: 0.6648\n",
      "Epoch: 069, Loss: 0.5605 Val: 0.7106 Test: 0.7603\n",
      "Epoch: 070, Loss: 0.5610 Val: 0.7166 Test: 0.7463\n",
      "Epoch: 071, Loss: 0.5598 Val: 0.6608 Test: 0.6915\n",
      "Epoch: 072, Loss: 0.5531 Val: 0.7009 Test: 0.7143\n",
      "Epoch: 073, Loss: 0.5551 Val: 0.6741 Test: 0.6427\n",
      "Epoch: 074, Loss: 0.5693 Val: 0.6763 Test: 0.6869\n",
      "Epoch: 075, Loss: 0.5380 Val: 0.7148 Test: 0.7383\n",
      "Epoch: 076, Loss: 0.5448 Val: 0.7122 Test: 0.7155\n",
      "Epoch: 077, Loss: 0.5543 Val: 0.6578 Test: 0.6905\n",
      "Epoch: 078, Loss: 0.5491 Val: 0.6941 Test: 0.6967\n",
      "Epoch: 079, Loss: 0.5307 Val: 0.6941 Test: 0.6913\n",
      "Epoch: 080, Loss: 0.5269 Val: 0.6335 Test: 0.7224\n",
      "Epoch: 081, Loss: 0.5264 Val: 0.6611 Test: 0.6752\n",
      "Epoch: 082, Loss: 0.5305 Val: 0.6859 Test: 0.7415\n",
      "Epoch: 083, Loss: 0.5190 Val: 0.6772 Test: 0.7144\n",
      "Epoch: 084, Loss: 0.5281 Val: 0.7292 Test: 0.7304\n",
      "Epoch: 085, Loss: 0.5252 Val: 0.8089 Test: 0.7308\n",
      "Epoch: 086, Loss: 0.5162 Val: 0.6914 Test: 0.6863\n",
      "Epoch: 087, Loss: 0.5480 Val: 0.6964 Test: 0.6900\n",
      "Epoch: 088, Loss: 0.5097 Val: 0.7141 Test: 0.7083\n",
      "Epoch: 089, Loss: 0.5067 Val: 0.6524 Test: 0.6624\n",
      "Epoch: 090, Loss: 0.4825 Val: 0.6323 Test: 0.7245\n",
      "Epoch: 091, Loss: 0.4944 Val: 0.7191 Test: 0.7214\n",
      "Epoch: 092, Loss: 0.5042 Val: 0.6682 Test: 0.7257\n",
      "Epoch: 093, Loss: 0.4873 Val: 0.6221 Test: 0.6895\n",
      "Epoch: 094, Loss: 0.5079 Val: 0.5911 Test: 0.7207\n",
      "Epoch: 095, Loss: 0.5024 Val: 0.6923 Test: 0.6965\n",
      "Epoch: 096, Loss: 0.4859 Val: 0.6386 Test: 0.6925\n",
      "Epoch: 097, Loss: 0.5033 Val: 0.6367 Test: 0.7009\n",
      "Epoch: 098, Loss: 0.5141 Val: 0.6580 Test: 0.7014\n",
      "Epoch: 099, Loss: 0.4747 Val: 0.7135 Test: 0.7083\n",
      "Epoch: 100, Loss: 0.5122 Val: 0.7351 Test: 0.7995\n",
      "Epoch: 101, Loss: 0.5216 Val: 0.6878 Test: 0.6812\n",
      "Epoch: 102, Loss: 0.4784 Val: 0.7204 Test: 0.7879\n",
      "Epoch: 103, Loss: 0.5044 Val: 0.6766 Test: 0.7809\n",
      "Epoch: 104, Loss: 0.4908 Val: 0.6819 Test: 0.6407\n",
      "Epoch: 105, Loss: 0.4801 Val: 0.6849 Test: 0.7297\n",
      "Epoch: 106, Loss: 0.4551 Val: 0.6012 Test: 0.7021\n",
      "Epoch: 107, Loss: 0.4664 Val: 0.6332 Test: 0.8036\n",
      "Epoch: 108, Loss: 0.4735 Val: 0.6290 Test: 0.7577\n",
      "Epoch: 109, Loss: 0.5028 Val: 0.6741 Test: 0.6859\n",
      "Epoch: 110, Loss: 0.4786 Val: 0.6913 Test: 0.6927\n",
      "Epoch: 111, Loss: 0.5033 Val: 0.6829 Test: 0.7153\n",
      "Epoch: 112, Loss: 0.4512 Val: 0.6880 Test: 0.6981\n",
      "Epoch: 113, Loss: 0.4637 Val: 0.6370 Test: 0.7358\n",
      "Epoch: 114, Loss: 0.4659 Val: 0.6746 Test: 0.6419\n",
      "Epoch: 115, Loss: 0.4487 Val: 0.6607 Test: 0.7742\n",
      "Epoch: 116, Loss: 0.4490 Val: 0.7074 Test: 0.7012\n",
      "Epoch: 117, Loss: 0.4650 Val: 0.6839 Test: 0.6941\n",
      "Epoch: 118, Loss: 0.4721 Val: 0.6648 Test: 0.6883\n",
      "Epoch: 119, Loss: 0.4295 Val: 0.6992 Test: 0.6997\n",
      "Epoch: 120, Loss: 0.4538 Val: 0.6674 Test: 0.7112\n",
      "Epoch: 121, Loss: 0.4534 Val: 0.7211 Test: 0.7069\n",
      "Epoch: 122, Loss: 0.4729 Val: 0.7711 Test: 0.7814\n",
      "Epoch: 123, Loss: 0.4700 Val: 0.7738 Test: 0.7440\n",
      "Epoch: 124, Loss: 0.4478 Val: 0.6638 Test: 0.6888\n",
      "Epoch: 125, Loss: 0.4572 Val: 0.6030 Test: 0.6787\n",
      "Epoch: 126, Loss: 0.4552 Val: 0.6391 Test: 0.6729\n",
      "Epoch: 127, Loss: 0.4658 Val: 0.6363 Test: 0.6714\n",
      "Epoch: 128, Loss: 0.5060 Val: 0.6841 Test: 0.7578\n",
      "Epoch: 129, Loss: 0.4672 Val: 0.6749 Test: 0.7247\n",
      "Epoch: 130, Loss: 0.4679 Val: 0.5968 Test: 0.6482\n",
      "Epoch: 131, Loss: 0.4562 Val: 0.6703 Test: 0.7151\n",
      "Epoch: 132, Loss: 0.4515 Val: 0.6475 Test: 0.6657\n",
      "Epoch: 133, Loss: 0.4636 Val: 0.6302 Test: 0.6803\n",
      "Epoch: 134, Loss: 0.4514 Val: 0.6008 Test: 0.6847\n",
      "Epoch: 135, Loss: 0.4648 Val: 0.6938 Test: 0.6644\n",
      "Epoch: 136, Loss: 0.4309 Val: 0.6924 Test: 0.6180\n",
      "Epoch: 137, Loss: 0.4217 Val: 0.6697 Test: 0.6772\n",
      "Epoch: 138, Loss: 0.4382 Val: 0.6586 Test: 0.6941\n",
      "Epoch: 139, Loss: 0.4349 Val: 0.6377 Test: 0.6495\n",
      "Epoch: 140, Loss: 0.4610 Val: 0.6373 Test: 0.6653\n",
      "Epoch: 141, Loss: 0.4502 Val: 0.7183 Test: 0.7285\n",
      "Epoch: 142, Loss: 0.4397 Val: 0.6085 Test: 0.7156\n",
      "Epoch: 143, Loss: 0.4343 Val: 0.7007 Test: 0.8767\n",
      "Epoch: 144, Loss: 0.4647 Val: 0.7215 Test: 0.7415\n",
      "Epoch: 145, Loss: 0.4391 Val: 0.6499 Test: 0.6668\n",
      "Epoch: 146, Loss: 0.4375 Val: 0.6314 Test: 0.6887\n",
      "Epoch: 147, Loss: 0.4327 Val: 0.6434 Test: 0.7012\n",
      "Epoch: 148, Loss: 0.4194 Val: 0.6561 Test: 0.7048\n",
      "Epoch: 149, Loss: 0.4338 Val: 0.7024 Test: 0.6709\n",
      "Epoch: 150, Loss: 0.4377 Val: 0.6439 Test: 0.6786\n",
      "Epoch: 151, Loss: 0.4220 Val: 0.6742 Test: 0.6555\n",
      "Epoch: 152, Loss: 0.4205 Val: 0.6225 Test: 0.6476\n",
      "Epoch: 153, Loss: 0.4251 Val: 0.6438 Test: 0.6720\n",
      "Epoch: 154, Loss: 0.4181 Val: 0.6585 Test: 0.6657\n",
      "Epoch: 155, Loss: 0.4207 Val: 0.6072 Test: 0.7327\n",
      "Epoch: 156, Loss: 0.4286 Val: 0.6351 Test: 0.6900\n",
      "Epoch: 157, Loss: 0.4220 Val: 0.6621 Test: 0.6734\n",
      "Epoch: 158, Loss: 0.4227 Val: 0.6311 Test: 0.6687\n",
      "Epoch: 159, Loss: 0.4089 Val: 0.6515 Test: 0.6336\n",
      "Epoch: 160, Loss: 0.4297 Val: 0.6699 Test: 0.6412\n",
      "Epoch: 161, Loss: 0.4228 Val: 0.6272 Test: 0.6684\n",
      "Epoch: 162, Loss: 0.4238 Val: 0.6302 Test: 0.6356\n",
      "Epoch: 163, Loss: 0.4309 Val: 0.6621 Test: 0.6926\n",
      "Epoch: 164, Loss: 0.4350 Val: 0.5910 Test: 0.6520\n",
      "Epoch: 165, Loss: 0.4256 Val: 0.6906 Test: 0.7330\n",
      "Epoch: 166, Loss: 0.4289 Val: 0.7217 Test: 0.6913\n",
      "Epoch: 167, Loss: 0.4226 Val: 0.6504 Test: 0.7458\n",
      "Epoch: 168, Loss: 0.4290 Val: 0.7033 Test: 0.7404\n",
      "Epoch: 169, Loss: 0.4042 Val: 0.7589 Test: 0.7341\n",
      "Epoch: 170, Loss: 0.4404 Val: 0.7330 Test: 0.6520\n",
      "Epoch: 171, Loss: 0.4037 Val: 0.7009 Test: 0.6492\n",
      "Epoch: 172, Loss: 0.3938 Val: 0.6262 Test: 0.6277\n",
      "Epoch: 173, Loss: 0.4316 Val: 0.6775 Test: 0.6582\n",
      "Epoch: 174, Loss: 0.4328 Val: 0.6274 Test: 0.7237\n",
      "Epoch: 175, Loss: 0.4051 Val: 0.6929 Test: 0.6625\n",
      "Epoch: 176, Loss: 0.4318 Val: 0.6979 Test: 0.6950\n",
      "Epoch: 177, Loss: 0.4139 Val: 0.6489 Test: 0.6757\n",
      "Epoch: 178, Loss: 0.4285 Val: 0.6740 Test: 0.6751\n",
      "Epoch: 179, Loss: 0.3893 Val: 0.6532 Test: 0.7446\n",
      "Epoch: 180, Loss: 0.3965 Val: 0.6411 Test: 0.6382\n",
      "Epoch: 181, Loss: 0.3940 Val: 0.6229 Test: 0.7204\n",
      "Epoch: 182, Loss: 0.4017 Val: 0.6414 Test: 0.6392\n",
      "Epoch: 183, Loss: 0.3850 Val: 0.6838 Test: 0.6577\n",
      "Epoch: 184, Loss: 0.4051 Val: 0.6786 Test: 0.6846\n",
      "Epoch: 185, Loss: 0.4089 Val: 0.7295 Test: 0.7750\n",
      "Epoch: 186, Loss: 0.4148 Val: 0.6775 Test: 0.7159\n",
      "Epoch: 187, Loss: 0.4240 Val: 0.6799 Test: 0.6867\n",
      "Epoch: 188, Loss: 0.3997 Val: 0.6722 Test: 0.6981\n",
      "Epoch: 189, Loss: 0.4242 Val: 0.5950 Test: 0.6849\n",
      "Epoch: 190, Loss: 0.3914 Val: 0.7487 Test: 0.6678\n",
      "Epoch: 191, Loss: 0.4047 Val: 0.6051 Test: 0.6741\n",
      "Epoch: 192, Loss: 0.3941 Val: 0.6479 Test: 0.6554\n",
      "Epoch: 193, Loss: 0.3896 Val: 0.6718 Test: 0.6937\n",
      "Epoch: 194, Loss: 0.3857 Val: 0.7273 Test: 0.7093\n",
      "Epoch: 195, Loss: 0.3864 Val: 0.6731 Test: 0.6717\n",
      "Epoch: 196, Loss: 0.3975 Val: 0.6231 Test: 0.6956\n",
      "Epoch: 197, Loss: 0.3867 Val: 0.6347 Test: 0.7066\n",
      "Epoch: 198, Loss: 0.3940 Val: 0.6248 Test: 0.6935\n",
      "Epoch: 199, Loss: 0.3775 Val: 0.6089 Test: 0.6428\n",
      "Epoch: 200, Loss: 0.3664 Val: 0.6529 Test: 0.7242\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    train_rmse = train()\n",
    "    val_rmse   = test(val_loader)\n",
    "    test_rmse  = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
    "          f'Test: {test_rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
